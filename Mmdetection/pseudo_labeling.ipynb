{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_json_data_path = \"/opt/ml/detection/dataset/train.json\" # 경로수정 필요\n",
    "with open(labeled_json_data_path) as f:\n",
    "\tlabeled_data = json.load(f)\n",
    "\n",
    "df_images = json_normalize(labeled_data['images'])\n",
    "# image_id  category_id     area    bbox    iscrowd     id\n",
    "df_annotations = json_normalize(labeled_data['annotations'])\n",
    "\n",
    "# 마지막 요소의 값들 가져오기\n",
    "# [1024 1024 'train/4882.jpg' 0 None None '2020-12-23 16:20:30' 4882]\n",
    "width, height, _, license, flickr_url, coco_url, date_captured, image_id_1 = df_images.tail(1).values[0]\n",
    "# [4882 1 149633.22 list([145.4, 295.4, 420.2, 356.1]) 0 23143]\n",
    "image_id_2, category_id, area, bbox, iscrowd, anno_id = df_annotations.tail(1).values[0]\n",
    "\n",
    "# 예측을 통해 나온 데이터 (Unlabeled data) ######################################################################\n",
    "submission_csv = '/opt/ml/detection/baseline/mmdetection/work_dirs/swinL/submission_latest.csv' # 경로수정 필요\n",
    "data = pd.read_csv(submission_csv, keep_default_na=False)\n",
    "data = data.values.tolist()\n",
    "# print(data.head(5)) # class, confidence, x1, y1, x2, y2 형태\n",
    "\n",
    "unlabeled = dict() # json 변환을 위한 dictionary\n",
    "unlabeled['images'] = []\n",
    "unlabeled['annotations'] = []\n",
    "confidence_threshold = 0.7\n",
    "\n",
    "for predict, image in data:\n",
    "    if predict == None: # 예측하지 못한 데이터는 pass\n",
    "        continue\n",
    "    predict = predict.strip() # 띄어쓰기만 있는 경우가 있을 수 있음\n",
    "    if predict == '': # 예측하지 못한 데이터는 pass\n",
    "        continue\n",
    "\n",
    "    count = 0 # annotation 개수 체크\n",
    "    split_predict = predict.split(' ')\n",
    "    anns_length = len(split_predict) // 6 # annotation 개수\n",
    "    image_save = False # 이미지 저장 여부\n",
    "    temp_image = dict()\n",
    "    temp_annotation = dict()\n",
    "    for i in range(anns_length):\n",
    "        class_ = int(split_predict[i*6])\n",
    "        confidence = float(split_predict[(i*6)+1])\n",
    "        Left = float(split_predict[(i*6)+2])\n",
    "        Top = float(split_predict[(i*6)+3])\n",
    "        Right = float(split_predict[(i*6)+4])\n",
    "        Bottom = float(split_predict[(i*6)+5])\n",
    "        Width = Right - Left\n",
    "        Height = Bottom - Top\n",
    "        Area = round(Width * Height, 2)\n",
    "        if confidence_threshold != None: # confidence Threshold 걸은 경우\n",
    "            if confidence < confidence_threshold:\n",
    "                continue\n",
    "        \n",
    "        # Image 추가\n",
    "        if image_save == False: # 추가된 이미지인지 확인\n",
    "            image_id_2 += 1\n",
    "            temp_image['width'] = width # 마지막 데이터 그대로 이용\n",
    "            temp_image['height'] = height # 마지막 데이터 그대로 이용\n",
    "            temp_image['file_name'] = image\n",
    "            temp_image['license'] = license # 마지막 데이터 그대로 이용\n",
    "            temp_image['flickr_url'] = flickr_url # 마지막 데이터 그대로 이용\n",
    "            temp_image['coco_url'] = coco_url # 마지막 데이터 그대로 이용\n",
    "            temp_image['date_captured'] = date_captured # 마지막 데이터 그대로 이용\n",
    "            temp_image['id'] = image_id_2\n",
    "            image_save = True\n",
    "\n",
    "        # Annotation 추가\n",
    "        anno_id += 1\n",
    "        count += 1\n",
    "        temp_annotation['image_id'] = image_id_2\n",
    "        temp_annotation['category_id'] = class_\n",
    "        temp_annotation['area'] = Area\n",
    "        temp_annotation['bbox'] = [round(Left, 1), round(Top, 1), round(Width, 1), round(Height, 1)]\n",
    "        temp_annotation['iscrowd'] = iscrowd # 마지막 데이터 그대로 이용\n",
    "        temp_annotation['id'] = anno_id\n",
    "\n",
    "    if count > 0: # 주석이 그려진게 있다면\n",
    "        unlabeled['images'].append(temp_image)\n",
    "        unlabeled['annotations'].append(temp_annotation)\n",
    "\n",
    "# Labeled Data + Unlabeled Data ################################################################################\n",
    "labeled_json_data2_path = \"/opt/ml/detection/dataset/train_aug_correct.json\" # 경로수정 필요\n",
    "with open(labeled_json_data2_path) as f:\n",
    "\tlabeled_data2 = json.load(f)\n",
    "\n",
    "labeled_data['images'] += unlabeled['images']\n",
    "labeled_data['annotations'] += unlabeled['annotations']\n",
    "# labeled_data['images'] += labeled_data2['images']\n",
    "# labeled_data['annotations'] += labeled_data2['annotations']\n",
    "    \n",
    "    \n",
    "with open(\"./train_new.json\", \"w\") as new_file:\n",
    "\tjson.dump(labeled_data, new_file)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2d617e4e4c71290b58af84e5aa30aeb780f45b049d7dc2bd64c8742d6fa340d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('openmlab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
